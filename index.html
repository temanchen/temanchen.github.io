<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tianwen&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Tianwen's blog">
<meta property="og:url" content="http://www.twchen.cc/index.html">
<meta property="og:site_name" content="Tianwen's blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tianwen's blog">
  
    <link rel="alternate" href="/atom.xml" title="Tianwen&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Tianwen&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://www.twchen.cc"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Neural-Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/21/Neural-Network/" class="article-date">
  <time datetime="2017-02-21T00:06:04.000Z" itemprop="datePublished">2017-02-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/21/Neural-Network/">Neural Network</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="notations">Notations</h3>
<table>
<thead>
<tr class="header">
<th align="center">Notation</th>
<th align="center">Meaning</th>
<th align="center">Dimension</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(L\)</span></td>
<td align="center">No. of layers, including the input/output layers</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(s_l\)</span></td>
<td align="center">No. of units in layer <span class="math inline">\(l\)</span></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(g(\cdot)\)</span></td>
<td align="center">The activation function</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(J_{\theta}(X)\)</span></td>
<td align="center">The cost function</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\((x, y)\)</span></td>
<td align="center">A sample</td>
<td align="center"><span class="math inline">\(s_1 \times 1\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(a^{(l)}\)</span></td>
<td align="center">The output of layer <span class="math inline">\(l\)</span></td>
<td align="center"><span class="math inline">\(s_l \times 1\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(z^{(l)}\)</span></td>
<td align="center">The input of activation function in layer <span class="math inline">\(l+1\)</span></td>
<td align="center"><span class="math inline">\(s_{l+1} \times 1\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(b^{(l)}\)</span></td>
<td align="center">The biased term in layer <span class="math inline">\(l\)</span></td>
<td align="center"><span class="math inline">\(s_{l+1} \times 1\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\theta^{(l)}\)</span></td>
<td align="center">The weights from layer <span class="math inline">\(l\)</span> to layer <span class="math inline">\(l+1\)</span></td>
<td align="center"><span class="math inline">\(s_{l+1} \times s_l\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\delta^{(l)}\)</span></td>
<td align="center">The <span class="math inline">\(\delta\)</span> vector in layer <span class="math inline">\(l\)</span></td>
<td align="center"><span class="math inline">\(s_l \times 1\)</span></td>
</tr>
</tbody>
</table>
<h3 id="derivation-of-the-delta-rule">Derivation of the delta rule</h3>
<p>First, consider the relationship among <span class="math inline">\(a, z, \theta\)</span> <span class="math display">\[
a^{(1)} = x \\
z^{(l)} = \theta^{(l)} a^{(l)} + b^{(l)} \text{, for } 1 \leq l \leq L-1 \\
a^{(l+1)} = g(z^{(l)}) \text{, for } 1 \leq l \leq L-1 \\
h_\theta(x) = a^{(L)} \\
\]</span> To update the weights, we need to compute the gradient <span class="math display">\[
\begin{align}
{\partial J_{\theta}(X) \over \partial \theta_{ji}^{(l)}} &amp;= {\partial J_{\theta}(X) \over \partial a_j^{(l+1)}}
{\partial a_j^{(l+1)} \over \partial z_j^{(l)}} {\partial z_j^{(l)} \over \partial \theta_{ji}^{(l)}} \\
&amp;= {\partial J_{\theta}(X) \over \partial a_j^{(l+1)}} {\partial a_j^{(l+1)} \over \partial z_j^{(l)}} a_i^{(l)}
\end{align} \\
\text{Define } \delta_j^{(l)} =
{\partial J_{\theta}(X) \over \partial a_j^{(l)}} {\partial a_j^{(l)} \over \partial z_j^{(l-1)}} \\
\text{Then } {\partial J_{\theta}(X) \over \partial \theta_{ji}^{(l)}} = \delta_j^{(l+1)} a_i^{(l)} \\
\]</span></p>
<h5 id="the-recursive-relationship-of-deltal">The recursive relationship of <span class="math inline">\(\delta^{(l)}\)</span></h5>
<p><span class="math display">\[
\text{When } 2 \leq l \leq L-1 \\
\begin{align}
\delta_j^{(l)} &amp;= {\partial J_{\theta}(X) \over \partial a_j^{(l)}} {\partial a_j^{(l)} \over \partial z_j^{(l-1)}} \\
&amp;= \left( \sum_{k=1}^{s_{l+1}} {\partial J_{\theta}(X) \over \partial a_k^{(l+1)}}
{\partial a_k^{(l+1)} \over \partial a_j^{(l)}} \right) {\partial a_j^{(l)} \over \partial z_j^{(l-1)}}
&amp;&amp; \text{$J$ depends on $a_j^{(l)}$ through $a_k^{(l+1)}, 1 \leq k \leq s_{l+1}$} \\
&amp;= \left( \sum_{k=1}^{s_{l+1}} {\partial J_{\theta}(X) \over \partial a_k^{(l+1)}}
{\partial a_k^{(l+1)} \over \partial z_k^{(l)}} {\partial z_k^{(l)} \over \partial a_j^{(l)}} \right) {\partial a_j^{(l)} \over \partial z_j^{(l-1)}} \\
&amp;= \left( \sum_{k=1}^{s_{l+1}} \delta_k^{(l+1)} \theta_{kj}^{(l)} \right) {\partial a_j^{(l)} \over \partial z_j^{(l-1)}}
&amp;&amp; \text{ ${\partial z_k^{(l)} \over \partial a_j^{(l)}} = \theta_{kj}^{(l)}$ }
\end{align}
\]</span></p>
<p>Thus, the gradient in vector form is as follows <span class="math display">\[
{\partial J_{\theta}(X) \over \partial \theta^{(l)}} = \delta^{(l+1)} a^{(l)T} \\
\text{where, } \\
\delta^{(L)} = {\partial J_{\theta}(X) \over \partial a^{(L)}} \cdot {\partial a^{(L)} \over \partial z^{(L-1)}} \\
\delta^{(l)} = \left( \theta^{(l)T} \delta^{(l+1)} \right) \cdot {\partial a^{(l)} \over \partial z^{(l-1)}}
\text{, for } 2 \leq l \leq L-1 \\
\]</span></p>
<p>Similarly, we can derive the gradient of the biased terms. (Or consider <span class="math inline">\(b_j^{(l)}\)</span> as <span class="math inline">\(\theta_{j0}^{(l)}\)</span>, and define <span class="math inline">\(a_0^{(l)} = 1\)</span>) <span class="math display">\[
{\partial J_{\theta}(X) \over \partial b^{(l)}} = \delta^{(l+1)}
\]</span></p>
<h3 id="example">Example</h3>
<p>Use entropy loss function and sigmoid activation function <span class="math display">\[
J_{\theta}(X) = - {1 \over m} \sum_{i=1}^m \sum_{o=1}^{s_L} 
[ y_o^{(i)} \log h_\theta(x^{(i)})_o + ( 1 - y_o^{(i)}) \log (1 - h_\theta(x^{(i)})_o) ] \\
g(z) = {1 \over 1 + e^{-z}} \\
\text{Then } \\
\delta^{(L)} = (h_{\theta}(x^{(i)}) - y^{(i)}) \\
\delta^{(l)} = \left( \theta^{(l)T} \delta^{(l+1)} \right) \cdot [a^{(l)} \cdot (1 - a^{(l)})]
\text{, for } 2 \leq l \leq L-1
\]</span></p>
<h3 id="python-implementation">Python implementation</h3>
<p><a href="https://github.com/temanchen/notebooks/blob/master/neural_network.ipynb" target="_blank" rel="external">Neural Network Notebook</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.twchen.cc/2017/02/21/Neural-Network/" data-id="cj6uwulbd0000wyphy4zbli7o" class="article-share-link">Share</a>
      
        <a href="http://www.twchen.cc/2017/02/21/Neural-Network/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Traveling-Salesman-Problem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/14/Traveling-Salesman-Problem/" class="article-date">
  <time datetime="2017-02-14T03:12:01.000Z" itemprop="datePublished">2017-02-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/14/Traveling-Salesman-Problem/">Traveling Salesman Problem</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="problem-metric-tsp-without-repetitions">Problem (Metric TSP without repetitions)</h3>
<p>Input: a complete, weighted graph <span class="math inline">\(G = (V, w)\)</span>. The weights are non-negative and satisfy the triangle inequality.<br>
Output: a minimum cost cycle that visits each vertex exactly once.</p>
<h3 id="approximation-algorithm">2-approximation Algorithm</h3>
<ol style="list-style-type: decimal">
<li>Create a MST <span class="math inline">\(T\)</span> of <span class="math inline">\(G\)</span>.<br>
</li>
<li>Apply DFS on <span class="math inline">\(T\)</span> to get a twice-around tour <span class="math inline">\(T_{DFS}\)</span>.<br>
</li>
<li>Skip the repeated vertices (<em>shortcutting</em>).</li>
</ol>
<h5 id="proof">Proof:</h5>
<p>Let <span class="math inline">\(T_{appr}\)</span> and <span class="math inline">\(T_{opt}\)</span> be the tour generated by the above algorithm and one optimal TSP tour respectively. Then <span class="math display">\[
\begin{align}
cost(T) &amp;\le cost(T_{opt}) &amp;&amp; \text{Removing an edge in $T_{opt}$ produces a spanning tree} \\
cost(T_{DFS}) &amp;= 2 \times cost(T) &amp;&amp; \text{Each edges in MST is traveled twice} \\
cost(T_{appr}) &amp;\le cost(T_{DFS}) &amp;&amp; \text{Shortcuttin does not increase the cost} \\
\end{align}
\]</span> Therefore, <span class="math inline">\(cost(T_{appr}) \le cost(T_{DFS}) = 2 \times cost(T) \le 2 \times cost(T_{opt})\)</span></p>
<h3 id="over-2-approximation-algorithm-christofides-algorithm"><span class="math inline">\({3 \over 2}\)</span>-approximation Algorithm (Christofides Algorithm)</h3>
<ol style="list-style-type: decimal">
<li>Create a MST <span class="math inline">\(T\)</span> of <span class="math inline">\(G\)</span>.</li>
<li>Let <span class="math inline">\(V_O\)</span> be the set of vertices with odd degree in <span class="math inline">\(T\)</span>. Since <span class="math inline">\(\left| V_O \right|\)</span> is even, we can find a <strong>minimum-weight perfect matching</strong> <span class="math inline">\(M\)</span> of <span class="math inline">\(V_O\)</span>.<br>
</li>
<li>Combine the edges of <span class="math inline">\(M\)</span> and <span class="math inline">\(T\)</span> to form a connected <strong>multigraph</strong> <span class="math inline">\(H\)</span>. (If an edge is in both <span class="math inline">\(M\)</span> and <span class="math inline">\(T\)</span>, then the edge is appreared twice in <span class="math inline">\(H\)</span>.)</li>
<li>Now each vertex in <span class="math inline">\(H\)</span> has even degree. We can form an <strong>Eulerian tour</strong> <span class="math inline">\(T_{e}\)</span> in <span class="math inline">\(H\)</span>.</li>
<li>Skip the repeated vertices (shortcutting) in <span class="math inline">\(T_{e}\)</span> to get a <strong>Hamiltonian tour</strong> <span class="math inline">\(T_{h}\)</span>, which is what we are looking for.</li>
</ol>
<h5 id="some-definitions-and-facts">Some definitions and facts:</h5>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\left| V_O \right|\)</span> is even<br>
Let <span class="math inline">\(V_E\)</span> be the set of vertices with even degree. <span class="math display">\[
2 \times \text{No. of edges in G} = \sum_{v \in V} deg(v) = \sum_{v_o \in V_O} deg(v_o) + \sum_{v_e \in V_E} deg(v_e)
\]</span> Since <span class="math inline">\(2 \times \text{No. of edges in G}\)</span> and <span class="math inline">\(deg(v_e)\)</span> are even numbers, <span class="math inline">\(\sum_{v_o \in V_O} deg(v_o)\)</span> is also an even number. Since <span class="math inline">\(deg(v_o)\)</span> are even numbers, there must be even number of <span class="math inline">\(v_o\)</span>, i.e. <span class="math inline">\(\left| V_O \right|\)</span> is even.</p></li>
<li><p>Minimum-weight perfect matching<br>
Definitions:<br>
<strong>Mathching</strong>: given a graph <span class="math inline">\(G=(V,E)\)</span>, a subset of edges <span class="math inline">\(M \subseteq E\)</span> is a matching if no two edges in <span class="math inline">\(M\)</span> share a common vertex.<br>
A <strong>perfect matching</strong> is a matching that covers all vertices in <span class="math inline">\(V\)</span>.<br>
A <strong>minimum-weight perfect matching</strong> is a perfect matching who has minimum total edge weights.</p></li>
<li><p>Multigraph A multigraph is a graph where multiple edges between vertices are allowed.</p></li>
<li><p>Eulerian tour and Hamiltonian tour<br>
An <strong>Eulerian tour</strong> of a graph is a cycle that uses every <strong>edge</strong> of the graph exactly once.<br>
A <strong>Hamiltonian tour</strong> of a graph is a cycle that visits each <strong>vertex</strong> of the graph exactly once.<br>
Theorem: A graph has an Eulerian tour <strong>iff</strong> every vertex has even degree and the vertices of positive degree are connected. Furthermore, there is a polynomial time algorithm to compute the Eulerian tour of such a graph.</p></li>
</ol>
<h5 id="proof-1">Proof:</h5>
<ol style="list-style-type: decimal">
<li>Get a cycle <span class="math inline">\(T_{odd}\)</span> from <span class="math inline">\(T_{opt}\)</span> by skipping the vertices with odd degree. <span class="math inline">\(cost(T_{odd}) \le cost(T_{opt})\)</span><br>
</li>
<li>Take the edges of <span class="math inline">\(T_{odd}\)</span> alternatively, we get two perfect matching <span class="math inline">\(M_1, M_2\)</span> of <span class="math inline">\(V_O\)</span>.<br>
Since <span class="math inline">\(M\)</span> is a minimum-weight perfect matching of <span class="math inline">\(V_O\)</span>, we have<br>
<span class="math inline">\(2 \times cost(M) \leq cost(M_1) + cost(M_2) = cost(T_{odd}) \Rightarrow cost(M) \leq {1 \over 2} cost(T_{opt})\)</span></li>
<li><span class="math display">\[
cost(T_{h}) \leq cost(T_{e}) \;\; \text{(shortcutting will not increse the cost)} \\
\leq cost(H) = cost(M) + cost(T) \\
\leq {1 \over 2} cost(T_{opt}) + cost(T_{opt})
\]</span></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.twchen.cc/2017/02/14/Traveling-Salesman-Problem/" data-id="cj6uwulbj0001wyph1u2nbrjb" class="article-share-link">Share</a>
      
        <a href="http://www.twchen.cc/2017/02/14/Traveling-Salesman-Problem/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Combinatorial-Optimization/">Combinatorial Optimization</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-softmax-regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/08/softmax-regression/" class="article-date">
  <time datetime="2017-02-08T06:48:08.000Z" itemprop="datePublished">2017-02-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/08/softmax-regression/">Softmax Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="what-is-softmax-regression">What is Softmax Regression</h3>
<p>Softmax regression is a generalization of logistic regression. It is used for <strong>multi-class classification</strong> under the assumption that the classes are <strong>independent</strong>. For each example <span class="math inline">\((x^{(i)},y^{(i)}),\,y^{(i)} \in \{1, 2, ..., K\}\)</span>, the model estimates <span class="math inline">\(p(y^{(i)}=k|x^{(i)},\theta)\)</span>.</p>
<h3 id="hypothesis">Hypothesis</h3>
<p><span class="math display">\[
h_{\theta}(x^{(i)})
= \begin{pmatrix}
p(y^{(i)}=1|x^{(i)},\theta) \\
p(y^{(i)}=2|x^{(i)},\theta) \\
\vdots \\
p(y^{(i)}=K|x^{(i)},\theta)
\end{pmatrix}
= \frac {1} {\sum_{k=1}^K e^{\theta_k^T x^{(i)}}}
\begin{pmatrix}
e^{\theta_1^T x^{(i)}} \\
e^{\theta_2^T x^{(i)}} \\
\vdots \\
e^{\theta_K^T x^{(i)}}
\end{pmatrix}
\]</span></p>
<h3 id="loss-function">Loss Function</h3>
<h5 id="entropy-loss-function">Entropy loss function</h5>
<p><span class="math display">\[
\begin{align}
J_{\theta}(X) &amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log p(y^{(i)}=k|x^{(i)},\theta) \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log \frac  {e^{\theta_k^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}}
\end{align}
\]</span></p>
<h3 id="gradient-of-loss-function">Gradient of loss function</h3>
<p><span class="math display">\[
\begin{align}
\text{Since }
J_{\theta}(X) &amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log \frac {e^{\theta_k^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \left[ \theta_k^T x^{(i)} - \log {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \right]
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\frac {\partial J_{\theta}(X)} {\partial \theta_{ab}} &amp;=
- \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \left[ 1_{\{y^{(i)}=a\}} x_b^{(i)} - \frac {e^{\theta_a^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} x_b{(i)} \right]
&amp;&amp; \text{($1_{\{y^{(i)}=a\}}x_b^{(i)}$ because the term $\theta_k^T x^{(i)}$ contributes to the gradient only when $k=a$)} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\}y^{(i)}=k\}} x_b^{(i)} \left[ 1_{\{y^{(i)}=a\}} - \frac {e^{\theta_a^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \right]
&amp;&amp; \text{(take out the common factor $x_b^{(i)}$)} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} x_b^{(i)} \left[ 1_{\{y^{(i)}=a\}} - p(y^{(i)}=a|x^{(i)},\theta) \right] \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} x_b^{(i)} \left[ 1_{\{y^{(i)}=a\}} - p(y^{(i)}=a|x^{(i)},\theta) \right]
&amp;&amp; \text{(the indicator $1_{\{y{i}=k\}}=1$ for exactly once)} \\
\end{align} \\
\text{Vectorized form and change a to k: }
\frac {\partial J_{\theta(X)}} {\partial \theta_{k}} =
- \frac {1} {m} \sum_{i=1}^{m} x^{(i)} \left[ 1_{\{y^{(i)}=k\}} - p(y^{(i)}=k|x^{(i)},\theta) \right]
\]</span></p>
<h3 id="property-of-softmax-regression-model-over-parameterization">Property of softmax regression model: over-parameterization</h3>
<p><span class="math display">\[
\text{Value of $h_{\theta}(x^{(i)})$ remains unchanged when we subtract any constant vector $\theta_0$ from all $\theta_k$} \\
\begin{align}
\frac {1} {\sum_{k=1}^K e^{(\theta_k - \theta_0)^T x^{(i)}}}
\begin{pmatrix}
e^{(\theta_1 - \theta_0)^T x^{(i)}} \\
e^{(\theta_2 - \theta_0)^T x^{(i)}} \\
\vdots \\
e^{(\theta_K - \theta_0)^T x^{(i)}}
\end{pmatrix}
&amp;= \frac {1} {\sum_{k=1}^K e^{\theta_k^T x^{(i)}}}
\begin{pmatrix}
e^{\theta_1^T x^{(i)}} \\
e^{\theta_2^T x^{(i)}} \\
\vdots \\
e^{\theta_K^T x^{(i)}}
\end{pmatrix}
\end{align}
\]</span></p>
<h3 id="relationship-to-logistic-regression">Relationship to logistic regression</h3>
<p>When <span class="math inline">\(K=2\)</span>, softmax regression reduces to logistic regression.<br>
Use softmax regression when classes are mutually exclusive.<br>
Use <span class="math inline">\(K\)</span> one-vs-all logistic regression models when classes are not mutually exclusive.</p>
<h3 id="python-implementation">Python implementation</h3>
<p>Let <span class="math inline">\(y^{(i)}\)</span> be a column vector of identity matrix <span class="math inline">\(I_K\)</span>. <span class="math display">\[
J_{\theta}(X) = - {1 \over m} \sum_{i=1}^m y^{(i)^T} \log {e^{\theta x^{(i)}} \over sum(e^{\theta x^{(i)}})} \\
{\partial J_{\theta}(X) \over \partial \theta} = 
\sum_{i=1}^m \left( {e^{\theta x^{(i)}} \over sum(e^{\theta x^{(i)}})} - y^{(i)} \right) x^{(i)^T} \\
\text{where } \theta = \begin{pmatrix} - \theta_1^T - \\ - \theta_2^T - \\ \vdots \\ - \theta_K^T - \end{pmatrix}
\]</span><br>
When <span class="math inline">\(\theta_k^T x^{i}\)</span> is large, the exponential function <span class="math inline">\(e^{\theta_k^T x^{i}}\)</span> may overflow. To prevent this, because of the overparameterization property of the hypothesis, we can subtract the maximum of <span class="math inline">\(\theta_k^T x^{i}\)</span> from each <span class="math inline">\(\theta_k^T x^{i}\)</span> before computing the exponential.<br>
<a href="https://github.com/temanchen/notebooks/blob/master/softmax_regression.ipynb" target="_blank" rel="external">Softmax regression notebook</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.twchen.cc/2017/02/08/softmax-regression/" data-id="cj6uwulbs0004wyph9hzopuea" class="article-share-link">Share</a>
      
        <a href="http://www.twchen.cc/2017/02/08/softmax-regression/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-linear_regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/03/linear_regression/" class="article-date">
  <time datetime="2017-02-03T09:45:01.000Z" itemprop="datePublished">2017-02-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/03/linear_regression/">Linear Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="problem">1.Problem</h3>
<p>Given a feature vector <span class="math inline">\(x\)</span>, predict a continuous value <span class="math inline">\(y\)</span></p>
<h3 id="hypothesis">2.Hypothesis</h3>
<p><span class="math inline">\(h_{\theta}(x) = \theta^{T}x\)</span><br>
Note for each input <span class="math inline">\(x\)</span>, we add a biased term <span class="math inline">\(x_0=1\)</span> for convenience.</p>
<h3 id="cost-function">3.Cost function</h3>
<p><span class="math display">\[
\begin{align}
J_{\theta}(X) &amp;= \dfrac {1} {2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \\
&amp;= \dfrac {1} {2m} \sum_{i=1}^m (\theta^{T}x^{(i)} - y^{(i)})^2
\end{align} \\
\text{where $(x^{(i)}, y^{(i)})$ is the $i^{th}$ training example.}
\]</span></p>
<h5 id="vectorized-form">Vectorized form:</h5>
<p><span class="math display">\[
J_{\theta}(X) = \dfrac {1} {2m} (X\theta - y)^T(X\theta - y) \\
\text{where } X = \begin{pmatrix}
-x^{(1)^T}- \\
-x^{(2)^T}- \\
\vdots \\
-x^{(m)^T}-
\end{pmatrix}
\text{, } y = \begin{pmatrix}
y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)}
\end{pmatrix}
\]</span></p>
<h3 id="gradient">4.Gradient</h3>
<p><span class="math display">\[
\begin{align}
\frac {\partial J_{\theta}(X)} {\partial \theta_j}
&amp;= \frac {1} {m} \sum_{i=1}^m(\theta^{T}x^{(i)} - y^{(i)})x_j^{(i)} \\
\frac {\partial J_{\theta}(X)} {\partial \theta} &amp;= \frac {1} {m} X^T(X\theta - y)
\end{align}
\]</span></p>
<h3 id="analytic-solution">5.Analytic solution</h3>
<p><span class="math display">\[
\leqalignno{
{\partial J_{\theta}(X)} \over {\partial \theta} &amp;= 0 \cr
{1 \over m} X^T(X\theta - y) &amp;= 0 &amp;\Leftrightarrow \cr
\theta &amp;= (X^TX)^{-1}X^Ty &amp;\Leftrightarrow \cr
}
\]</span> Note that if <span class="math inline">\(m \lt n\)</span> or there are some redundant features, then <span class="math inline">\(X^T X\)</span> is singular (non-invertible).</p>
<h5 id="proof">Proof:</h5>
<p>First letâ€™s define what is a redundant feature.<br>
Left <span class="math inline">\(x_i\)</span> be a <span class="math inline">\(m \times 1\)</span> vector representing the <span class="math inline">\(i^{th}\)</span> feature. If <span class="math inline">\(x_i\)</span> is a linear combination of other features, then <span class="math inline">\(x_i\)</span> is a redundant feature.<br>
If <span class="math inline">\(m \lt n\)</span>, since <span class="math inline">\(x_i \in R^m\)</span>, and there are <span class="math inline">\(n \gt m\)</span> of them, there must be some redundant features.<br>
If there are some redundant features, use <strong><em>outer product expansion</em></strong> to expand <span class="math inline">\(X^T X\)</span> (let <span class="math inline">\(X = \begin{pmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{pmatrix}\)</span>). It is then easy to show that some row of <span class="math inline">\(X^TX\)</span> is a linear combination of other rows of <span class="math inline">\(X^T X,\)</span> which means <span class="math inline">\(X^T X\)</span> is singular.</p>
<h3 id="python-implementation">6.Python implementation</h3>
<p><a href="https://github.com/temanchen/notebooks/blob/master/linear_regression.ipynb" target="_blank" rel="external">Linear Regression Notebook</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://www.twchen.cc/2017/02/03/linear_regression/" data-id="cj6uwulbq0003wyphs9cohzng" class="article-share-link">Share</a>
      
        <a href="http://www.twchen.cc/2017/02/03/linear_regression/#disqus_thread" class="article-comment-link">Comments</a>
      
      
    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Combinatorial-Optimization/">Combinatorial Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Network/">Neural Network</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Combinatorial-Optimization/" style="font-size: 10px;">Combinatorial Optimization</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a> <a href="/tags/Neural-Network/" style="font-size: 10px;">Neural Network</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/02/21/Neural-Network/">Neural Network</a>
          </li>
        
          <li>
            <a href="/2017/02/14/Traveling-Salesman-Problem/">Traveling Salesman Problem</a>
          </li>
        
          <li>
            <a href="/2017/02/08/softmax-regression/">Softmax Regression</a>
          </li>
        
          <li>
            <a href="/2017/02/03/linear_regression/">Linear Regression</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Tianwen CHEN<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    
<script>
  var disqus_shortname = 'twchen';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>


  </div>
</body>
</html>