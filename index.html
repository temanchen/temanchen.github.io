<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tianwen&#39;s blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Tianwen's blog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Tianwen's blog">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tianwen's blog">
  
    <link rel="alternate" href="/atom.xml" title="Tianwen&#39;s blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Tianwen&#39;s blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-Traveling-Salesman-Problem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/14/Traveling-Salesman-Problem/" class="article-date">
  <time datetime="2017-02-14T03:12:01.000Z" itemprop="datePublished">2017-02-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/14/Traveling-Salesman-Problem/">Traveling Salesman Problem</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="problem-metric-tsp-without-repetitions">Problem (Metric TSP without repetitions)</h3>
<p>Input: a complete, weighted graph <span class="math inline">\(G = (V, w)\)</span>. The weights are non-negative and satisfy the triangle inequality.<br>
Output: a minimum cost cycle that visits each vertex exactly once.</p>
<h3 id="approximation-algorithm">2-approximation Algorithm</h3>
<ol style="list-style-type: decimal">
<li>Create a MST <span class="math inline">\(T\)</span> of <span class="math inline">\(G\)</span>.<br>
</li>
<li>Apply DFS on <span class="math inline">\(T\)</span> to get a twice-around tour <span class="math inline">\(T_{DFS}\)</span>.<br>
</li>
<li>Skip the repeated vertices (<em>shortcutting</em>).</li>
</ol>
<h5 id="proof">Proof:</h5>
<p>Let <span class="math inline">\(T_{appr}\)</span> and <span class="math inline">\(T_{opt}\)</span> be the tour generated by the above algorithm and one optimal TSP tour respectively. Then <span class="math display">\[
\begin{align}
cost(T) &amp;\le cost(T_{opt}) &amp;&amp; \text{Removing an edge in $T_{opt}$ produces a spanning tree} \\
cost(T_{DFS}) &amp;= 2 \times cost(T) &amp;&amp; \text{Each edges in MST is traveled twice} \\
cost(T_{appr}) &amp;\le cost(T_{DFS}) &amp;&amp; \text{Shortcuttin does not increase the cost} \\
\end{align}
\]</span> Therefore, <span class="math inline">\(cost(T_{appr}) \le cost(T_{DFS}) = 2 \times cost(T) \le 2 \times cost(T_{opt})\)</span></p>
<h3 id="over-2-approximation-algorithm-christofides-algorithm"><span class="math inline">\({3 \over 2}\)</span>-approximation Algorithm (Christofides Algorithm)</h3>
<ol style="list-style-type: decimal">
<li>Create a MST <span class="math inline">\(T\)</span> of <span class="math inline">\(G\)</span>.</li>
<li>Let <span class="math inline">\(V_O\)</span> be the set of vertices with odd degree in <span class="math inline">\(T\)</span>. Since <span class="math inline">\(\left| V_O \right|\)</span> is even, we can find a <strong>minimum-weight perfect matching</strong> <span class="math inline">\(M\)</span> of <span class="math inline">\(V_O\)</span>.<br>
</li>
<li>Combine the edges of <span class="math inline">\(M\)</span> and <span class="math inline">\(T\)</span> to form a connected <strong>multigraph</strong> <span class="math inline">\(H\)</span>. (If an edge is in both <span class="math inline">\(M\)</span> and <span class="math inline">\(T\)</span>, then the edge is appreared twice in <span class="math inline">\(H\)</span>.)</li>
<li>Now each vertex in <span class="math inline">\(H\)</span> has even degree. We can form an <strong>Eulerian tour</strong> <span class="math inline">\(T_{e}\)</span> in <span class="math inline">\(H\)</span>.</li>
<li>Skip the repeated vertices (shortcutting) in <span class="math inline">\(T_{e}\)</span> to get a <strong>Hamiltonian tour</strong> <span class="math inline">\(T_{h}\)</span>, which is what we are looking for.</li>
</ol>
<h5 id="some-definitions-and-facts">Some definitions and facts:</h5>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\left| V_O \right|\)</span> is even<br>
Let <span class="math inline">\(V_E\)</span> be the set of vertices with even degree. <span class="math display">\[
2 \times \text{No. of edges in G} = \sum_{v \in V} deg(v) = \sum_{v_o \in V_O} deg(v_o) + \sum_{v_e \in V_E} deg(v_e)
\]</span> Since <span class="math inline">\(2 \times \text{No. of edges in G}\)</span> and <span class="math inline">\(deg(v_e)\)</span> are even numbers, <span class="math inline">\(\sum_{v_o \in V_O} deg(v_o)\)</span> is also an even number. Since <span class="math inline">\(deg(v_o)\)</span> are even numbers, there must be even number of <span class="math inline">\(v_o\)</span>, i.e. <span class="math inline">\(\left| V_O \right|\)</span> is even.</p></li>
<li><p>Minimum-weight perfect matching<br>
Definitions:<br>
<strong>Mathching</strong>: given a graph <span class="math inline">\(G=(V,E)\)</span>, a subset of edges <span class="math inline">\(M \subseteq E\)</span> is a matching if no two edges in <span class="math inline">\(M\)</span> share a common vertex.<br>
A <strong>perfect matching</strong> is a matching that covers all vertices in <span class="math inline">\(V\)</span>.<br>
A <strong>minimum-weight perfect matching</strong> is a perfect matching who has minimum total edge weights.</p></li>
<li><p>Multigraph A multigraph is a graph where multiple edges between vertices are allowed.</p></li>
<li><p>Eulerian tour and Hamiltonian tour<br>
An <strong>Eulerian tour</strong> of a graph is a cycle that uses every <strong>edge</strong> of the graph exactly once.<br>
A <strong>Hamiltonian tour</strong> of a graph is a cycle that visits each <strong>vertex</strong> of the graph exactly once.<br>
Theorem: A graph has an Eulerian tour <strong>iff</strong> every vertex has even degree and the vertices of positive degree are connected. Furthermore, there is a polynomial time algorithm to compute the Eulerian tour of such a graph.</p></li>
</ol>
<h5 id="proof-1">Proof:</h5>
<ol style="list-style-type: decimal">
<li>Get a cycle <span class="math inline">\(T_{odd}\)</span> from <span class="math inline">\(T_{opt}\)</span> by skipping the vertices with odd degree. <span class="math inline">\(cost(T_{odd}) \le cost(T_{opt})\)</span><br>
</li>
<li>Take the edges of <span class="math inline">\(T_{odd}\)</span> alternatively, we get two perfect matching <span class="math inline">\(M_1, M_2\)</span> of <span class="math inline">\(V_O\)</span>.<br>
Since <span class="math inline">\(M\)</span> is a minimum-weight perfect matching of <span class="math inline">\(V_O\)</span>, we have<br>
<span class="math inline">\(2 \times cost(M) \leq cost(M_1) + cost(M_2) = cost(T_{odd}) \Rightarrow cost(M) \leq {1 \over 2} cost(T_{opt})\)</span></li>
<li><span class="math display">\[
cost(T_{h}) \leq cost(T_{e}) \;\; \text{(shortcutting will not increse the cost)} \\
\leq cost(H) = cost(M) + cost(T) \\
\leq {1 \over 2} cost(T_{opt}) + cost(T_{opt})
\]</span></li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/02/14/Traveling-Salesman-Problem/" data-id="ciz9iriwa0000s0phvcky22ub" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Combinational-Optimization/">Combinational Optimization</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-softmax-regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/08/softmax-regression/" class="article-date">
  <time datetime="2017-02-08T06:48:08.000Z" itemprop="datePublished">2017-02-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/08/softmax-regression/">Softmax Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="what-is-softmax-regression">What is Softmax Regression</h3>
<p>Softmax regression is a generalization of logistic regression. It is used for <strong>multi-class classification</strong> under the assumption that the classes are <strong>independent</strong>. For each example <span class="math inline">\((x^{(i)},y^{(i)}),\,y^{(i)} \in \{1, 2, ..., K\}\)</span>, the model estimates <span class="math inline">\(p(y^{(i)}=k|x^{(i)},\theta)\)</span>.</p>
<h3 id="hypothesis">Hypothesis</h3>
<p><span class="math display">\[
h_{\theta}(x^{(i)})
= \begin{pmatrix}
p(y^{(i)}=1|x^{(i)},\theta) \\
p(y^{(i)}=2|x^{(i)},\theta) \\
\vdots \\
p(y^{(i)}=K|x^{(i)},\theta)
\end{pmatrix}
= \frac {1} {\sum_{k=1}^K e^{\theta_k^T x^{(i)}}}
\begin{pmatrix}
e^{\theta_1^T x^{(i)}} \\
e^{\theta_2^T x^{(i)}} \\
\vdots \\
e^{\theta_K^T x^{(i)}}
\end{pmatrix}
\]</span></p>
<h3 id="loss-function">Loss Function</h3>
<h5 id="entropy-loss-function">Entropy loss function</h5>
<p><span class="math display">\[
\begin{align}
J_{\theta}(X) &amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log p(y^{(i)}=k|x^{(i)},\theta) \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log \frac  {e^{\theta_k^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}}
\end{align}
\]</span></p>
<h3 id="gradient-of-loss-function">Gradient of loss function</h3>
<p><span class="math display">\[
\begin{align}
\text{Since }
J_{\theta}(X) &amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \log \frac {e^{\theta_k^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{\{y^{(i)}=k\}} \left[ \theta_k^T x^{(i)} - \log {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \right]
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
\frac {\partial J_{\theta}(X)} {\partial \theta_{st}} &amp;=
- \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{y^{(i)}=k} \left[ 1_{y^{(i)}=a}x_b^{(i)} - \frac {e^{\theta_a^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} x_b{(i)} \right]
&amp;&amp; \text{($1_{y^{(i)}=a}x_b^{(i)}$ because the term $\theta_k^T x^{(i)}$ contributes to the gradient only when $k=a$)} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{y^{(i)}=k} x_b^{(i)} \left[ 1_{y^{(i)}=a} - \frac {e^{\theta_a^T x^{(i)}}} {\sum_{l=1}^{K} e^{\theta_l^T x^{(i)}}} \right]
&amp;&amp; \text{(take out the common factor $x_b^{(i)}$)} \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} \sum_{k=1}^{K} 1_{y^{(i)}=k} x_b^{(i)} \left[ 1_{y^{(i)}=a} - p(y^{(i)}=a|x^{(i)},\theta) \right] \\
&amp;= - \frac {1} {m} \sum_{i=1}^{m} x_b^{(i)} \left[ 1_{y^{(i)}=a} - p(y^{(i)}=a|x^{(i)},\theta) \right]
&amp;&amp; \text{(the indicator $1_{\{y{i}=k\}}=1$ for exactly once)} \\
\end{align} \\
\text{Vectorized form and change a to k: }
\frac {\partial J_{\theta(X)}} {\partial \theta_{k}} =
- \frac {1} {m} \sum_{i=1}^{m} x^{(i)} \left[ 1_{y^{(i)}=k} - p(y^{(i)}=k|x^{(i)},\theta) \right]
\]</span></p>
<h3 id="property-of-softmax-regression-model-over-parameterization">Property of softmax regression model: over-parameterization</h3>
<p><span class="math display">\[
\text{Value of $h_{\theta}(x^{(i)})$ remains unchanged when we subtract any constant vector $\theta_0$ from all $\theta_k$} \\
\begin{align}
\frac {1} {\sum_{k=1}^K e^{(\theta_k - \theta_0)^T x^{(i)}}}
\begin{pmatrix}
e^{(\theta_1 - \theta_0)^T x^{(i)}} \\
e^{(\theta_2 - \theta_0)^T x^{(i)}} \\
\vdots \\
e^{(\theta_K - \theta_0)^T x^{(i)}}
\end{pmatrix}
&amp;= \frac {1} {\sum_{k=1}^K e^{\theta_k^T x^{(i)}}}
\begin{pmatrix}
e^{\theta_1^T x^{(i)}} \\
e^{\theta_2^T x^{(i)}} \\
\vdots \\
e^{\theta_K^T x^{(i)}}
\end{pmatrix}
\end{align}
\]</span></p>
<h3 id="relation-with-logistic-regression">Relation with logistic regression</h3>
<h5 id="proof-of-logistic-regression-loss-function-and-gradient">Proof of logistic regression loss function and gradient</h5>
<p>Use softmax regression when classes are mutually exclusive.<br>
Use <span class="math inline">\(K\)</span> one-vs-all logistic regression models when classes are not mutually exclusive.</p>
<h3 id="to-do">To do</h3>
<p>The loss function and gradient when <span class="math display">\[
y^{(i)} \in Y = \left\{
\begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{pmatrix},
\begin{pmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{pmatrix},
\ldots,
\begin{pmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{pmatrix}
\right\}. \text{ Each matrix in Y is of length K}
\]</span></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/02/08/softmax-regression/" data-id="ciz9iriwn0003s0ph4a3kv7gg" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-linear_regression" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/03/linear_regression/" class="article-date">
  <time datetime="2017-02-03T09:45:01.000Z" itemprop="datePublished">2017-02-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/02/03/linear_regression/">Linear Regression</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h3 id="problem">1.Problem</h3>
<p>Given a feature vector <span class="math inline">\(x\)</span>, predict a continuous value <span class="math inline">\(y\)</span></p>
<h3 id="hypothesis">2.Hypothesis</h3>
<p><span class="math inline">\(h_{\theta}(x) = \theta^{T}x\)</span><br>
Note for each input <span class="math inline">\(x\)</span>, we add a biased term <span class="math inline">\(x_0=1\)</span> for convenience.</p>
<h3 id="cost-function">3.Cost function</h3>
<p><span class="math display">\[
\begin{align}
J_{\theta}(X) &amp;= \dfrac {1} {2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \\
&amp;= \dfrac {1} {2m} \sum_{i=1}^m (\theta^{T}x^{(i)} - y^{(i)})^2
\end{align} \\
\text{where $(x^{(i)}, y^{(i)})$ is the $i^{th}$ training example.}
\]</span></p>
<h5 id="vectorized-form">Vectorized form:</h5>
<p><span class="math display">\[
J_{\theta}(X) = \dfrac {1} {2m} (X\theta - y)^T(X\theta - y) \\
\text{where } X = \begin{pmatrix}
-x^{(1)^T}- \\
-x^{(2)^T}- \\
\vdots \\
-x^{(m)^T}-
\end{pmatrix}
\text{, } y = \begin{pmatrix}
y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)}
\end{pmatrix}
\]</span></p>
<h3 id="gradient">4.Gradient</h3>
<p><span class="math display">\[
\begin{align}
\frac {\partial J_{\theta}(X)} {\partial \theta_j}
&amp;= \frac {1} {m} \sum_{i=1}^m(\theta^{T}x^{(i)} - y^{(i)})x_j^{(i)} \\
\frac {\partial J_{\theta}(X)} {\partial \theta} &amp;= \frac {1} {m} X^T(X\theta - y)
\end{align}
\]</span></p>
<h3 id="analytic-solution">5.Analytic solution</h3>
<p><span class="math display">\[
\leqalignno{
{\partial J_{\theta}(X)} \over {\partial \theta} &amp;= 0 \cr
{1 \over m} X^T(X\theta - y) &amp;= 0 &amp;\Leftrightarrow \cr
\theta &amp;= (X^TX)^{-1}X^Ty &amp;\Leftrightarrow \cr
}
\]</span> Note that if <span class="math inline">\(m \lt n\)</span> or there are some redundant features, then <span class="math inline">\(X^T X\)</span> is singular (non-invertible).<br>
##### Proof:<br>
First let’s define what is a redundant feature.<br>
Left <span class="math inline">\(x_i\)</span> be a <span class="math inline">\(m \times 1\)</span> vector representing the <span class="math inline">\(i^{th}\)</span> feature. If <span class="math inline">\(x_i\)</span> is a linear combination of other features, then <span class="math inline">\(x_i\)</span> is a redundant feature.<br>
If <span class="math inline">\(m \lt n\)</span>, since <span class="math inline">\(x_i \in R^m\)</span>, and there are <span class="math inline">\(n \gt m\)</span> of them, there must be some redundant features.<br>
If there are some redundant features, use <strong><em>outer product expansion</em></strong> to expand <span class="math inline">\(X^T X\)</span> (let <span class="math inline">\(X = \begin{pmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{pmatrix}\)</span>). It is then easy to show that some row of <span class="math inline">\(X^TX\)</span> is a linear combination of other rows of <span class="math inline">\(X^T X,\)</span> which means <span class="math inline">\(X^T X\)</span> is singular.</p>
<h3 id="python-implementation">6.Python implementation</h3>
<p><a href="">Linear Regression Notebook</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/02/03/linear_regression/" data-id="ciz9iriwg0001s0pha0tdi8or" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Combinational-Optimization/">Combinational Optimization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Combinational-Optimization/" style="font-size: 10px;">Combinational Optimization</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/02/">February 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/02/14/Traveling-Salesman-Problem/">Traveling Salesman Problem</a>
          </li>
        
          <li>
            <a href="/2017/02/08/softmax-regression/">Softmax Regression</a>
          </li>
        
          <li>
            <a href="/2017/02/03/linear_regression/">Linear Regression</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Tianwen CHEN<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>


  </div>
</body>
</html>